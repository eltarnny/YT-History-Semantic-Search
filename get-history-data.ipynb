{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of videos in your YouTube History and their transcripts and save them locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install requests\n",
    "! pip install pandas\n",
    "! pip install pyarrow\n",
    "! pip install youtube_transcript_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google API does not allow Histry data to be exported.  \n",
    "So, I used [Google Takeout](https://takeout.google.com/) to download the YouTube History in a JSON file.  \n",
    "Select only YouTube History and JSON format.  \n",
    "Open the watch-history.json, clean-up ads from records, clean-up titles, select a date to filter out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON data from a file\n",
    "with open('watch-history.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "# Filter out records with 'details' field (to clean-up ads)\n",
    "filtered_data = [record for record in data if 'details' not in record and record.get('titleUrl')]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(filtered_data)\n",
    "\n",
    "# Select required columns\n",
    "df = df[['title', 'titleUrl', 'time']]\n",
    "\n",
    "# Filter out rows where 'title' contains 'https' (to clean-up deleted videos)\n",
    "df = df[~df['title'].str.contains('https')]\n",
    "\n",
    "# Function to convert time to a uniform format\n",
    "def convert_time(time_str):\n",
    "    try:\n",
    "        return datetime.strptime(time_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    except ValueError:\n",
    "        return datetime.strptime(time_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "# Convert 'time' to datetime\n",
    "df['time'] = df['time'].apply(convert_time)\n",
    "# Filter out rows where 'time' is before '2024-04-17' (my cut-off point that I want to search in)\n",
    "df = df[df['time'] >= '2024-04-17']\n",
    "\n",
    "# Remove 'Watched ' and various bad characters from the titles\n",
    "replacements = {'Watched ':'',\"—\":\"-\",\"’\":\"'\",\"‘\":\"'\",\"“\":\"'\",\"”\":\"'\",\"â€”\":'-','â€™':\"'\",'â€œ':\"'\",'!â€':\"'\"}\n",
    "for key,value in replacements.items():\n",
    "    df['title'] = df['title'].str.replace(key, value)\n",
    "\n",
    "# Rename 'time' to 'datetime'\n",
    "df = df.rename(columns={'time': 'datetime'})\n",
    "\n",
    "# Keep only the most recent unique 'titleUrl'\n",
    "df = df.sort_values('datetime', ascending=False).drop_duplicates('titleUrl', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Parquet and CSV files\n",
    "df.to_parquet('output.parquet', index=False)\n",
    "df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the transcripts of the videos in your History and add it to the data. On my PC it took 5m30s for 450 records.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file\n",
    "df = pd.read_parquet('output.parquet')\n",
    "\n",
    "# Function to get transcript\n",
    "def get_transcript(url):\n",
    "    video_id = url.split('=')[-1]\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        transcript = ' '.join([i['text'] for i in transcript_list])\n",
    "    except:\n",
    "        transcript = None\n",
    "    return transcript\n",
    "\n",
    "# Apply the function to the 'titleUrl' column to create a new 'transcript' column\n",
    "df['transcript'] = df['titleUrl'].apply(get_transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write back to Parquet file\n",
    "df.to_parquet('output_with_transcripts.parquet', index=False)\n",
    "# CSV file has some problems with line breaks in the 'transcript' column. Parquet file works fine.\n",
    "df.to_csv('output_with_transcripts.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-YT-History",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
